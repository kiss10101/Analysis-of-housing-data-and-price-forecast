#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†å¸ƒå¼å­˜å‚¨ç³»ç»Ÿæ¼”ç¤º
æ¨¡æ‹ŸElasticsearchåˆ†å¸ƒå¼å­˜å‚¨å’Œæ£€ç´¢åŠŸèƒ½
"""

import json
import time
import random
from datetime import datetime
from typing import Dict, List, Any
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class MockElasticsearchNode:
    """æ¨¡æ‹ŸElasticsearchèŠ‚ç‚¹"""
    
    def __init__(self, node_id: str, port: int):
        self.node_id = node_id
        self.port = port
        self.data = {}  # å­˜å‚¨æ•°æ®
        self.shards = {}  # åˆ†ç‰‡æ•°æ®
        self.status = "green"
        self.start_time = datetime.now()
    
    def add_document(self, index: str, doc_id: str, document: Dict[str, Any]):
        """æ·»åŠ æ–‡æ¡£åˆ°èŠ‚ç‚¹"""
        if index not in self.data:
            self.data[index] = {}
        self.data[index][doc_id] = document
        logger.info(f"èŠ‚ç‚¹ {self.node_id} å­˜å‚¨æ–‡æ¡£: {doc_id}")
    
    def search_documents(self, index: str, query: str) -> List[Dict[str, Any]]:
        """åœ¨èŠ‚ç‚¹ä¸­æœç´¢æ–‡æ¡£"""
        results = []
        if index in self.data:
            for doc_id, doc in self.data[index].items():
                # ç®€å•çš„å…³é”®è¯åŒ¹é…
                if query.lower() in str(doc).lower():
                    results.append({
                        'id': doc_id,
                        'source': doc,
                        'node': self.node_id
                    })
        return results
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–èŠ‚ç‚¹ç»Ÿè®¡ä¿¡æ¯"""
        total_docs = sum(len(docs) for docs in self.data.values())
        return {
            'node_id': self.node_id,
            'port': self.port,
            'status': self.status,
            'total_documents': total_docs,
            'indices': list(self.data.keys()),
            'uptime': str(datetime.now() - self.start_time)
        }

class MockElasticsearchCluster:
    """æ¨¡æ‹ŸElasticsearché›†ç¾¤"""
    
    def __init__(self):
        self.nodes = [
            MockElasticsearchNode("node-1", 9200),
            MockElasticsearchNode("node-2", 9201),
            MockElasticsearchNode("node-3", 9202)
        ]
        self.cluster_name = "house-data-cluster"
        self.index_config = {
            "house_data": {
                "shards": 3,
                "replicas": 1
            }
        }
    
    def _get_shard_node(self, doc_id: str, shard_count: int) -> int:
        """è®¡ç®—æ–‡æ¡£åº”è¯¥å­˜å‚¨åœ¨å“ªä¸ªåˆ†ç‰‡ï¼ˆèŠ‚ç‚¹ï¼‰"""
        return hash(doc_id) % shard_count
    
    def index_document(self, index: str, doc_id: str, document: Dict[str, Any]):
        """ç´¢å¼•æ–‡æ¡£åˆ°é›†ç¾¤"""
        if index not in self.index_config:
            logger.error(f"ç´¢å¼• {index} ä¸å­˜åœ¨")
            return False
        
        config = self.index_config[index]
        
        # è®¡ç®—ä¸»åˆ†ç‰‡
        primary_shard = self._get_shard_node(doc_id, config["shards"])
        primary_node = self.nodes[primary_shard]
        
        # å­˜å‚¨åˆ°ä¸»åˆ†ç‰‡
        primary_node.add_document(index, doc_id, document)
        
        # å­˜å‚¨åˆ°å‰¯æœ¬åˆ†ç‰‡
        replica_node_idx = (primary_shard + 1) % len(self.nodes)
        replica_node = self.nodes[replica_node_idx]
        replica_node.add_document(f"{index}_replica", doc_id, document)
        
        logger.info(f"æ–‡æ¡£ {doc_id} å­˜å‚¨åˆ°ä¸»åˆ†ç‰‡(èŠ‚ç‚¹{primary_shard+1})å’Œå‰¯æœ¬åˆ†ç‰‡(èŠ‚ç‚¹{replica_node_idx+1})")
        return True
    
    def search(self, index: str, query: str) -> Dict[str, Any]:
        """åœ¨é›†ç¾¤ä¸­æœç´¢"""
        start_time = time.time()
        all_results = []
        
        # å¹¶è¡Œæœç´¢æ‰€æœ‰èŠ‚ç‚¹
        for node in self.nodes:
            node_results = node.search_documents(index, query)
            all_results.extend(node_results)
        
        # å»é‡ï¼ˆå› ä¸ºæœ‰å‰¯æœ¬ï¼‰
        unique_results = {}
        for result in all_results:
            doc_id = result['id']
            if doc_id not in unique_results:
                unique_results[doc_id] = result
        
        search_time = time.time() - start_time
        
        return {
            'took': int(search_time * 1000),  # æ¯«ç§’
            'hits': {
                'total': len(unique_results),
                'hits': list(unique_results.values())
            },
            'cluster_info': {
                'nodes_searched': len(self.nodes),
                'shards_searched': self.index_config.get(index, {}).get('shards', 0)
            }
        }
    
    def get_cluster_health(self) -> Dict[str, Any]:
        """è·å–é›†ç¾¤å¥åº·çŠ¶æ€"""
        total_docs = 0
        for node in self.nodes:
            stats = node.get_stats()
            total_docs += stats['total_documents']
        
        return {
            'cluster_name': self.cluster_name,
            'status': 'green',
            'number_of_nodes': len(self.nodes),
            'number_of_data_nodes': len(self.nodes),
            'active_primary_shards': sum(config['shards'] for config in self.index_config.values()),
            'active_shards': sum(config['shards'] * (1 + config['replicas']) for config in self.index_config.values()),
            'total_documents': total_docs,
            'nodes': [node.get_stats() for node in self.nodes]
        }
    
    def bulk_index(self, index: str, documents: List[Dict[str, Any]]):
        """æ‰¹é‡ç´¢å¼•æ–‡æ¡£"""
        logger.info(f"å¼€å§‹æ‰¹é‡ç´¢å¼• {len(documents)} ä¸ªæ–‡æ¡£åˆ° {index}")
        
        success_count = 0
        for doc in documents:
            doc_id = doc.get('id', str(random.randint(1000, 9999)))
            if self.index_document(index, doc_id, doc):
                success_count += 1
        
        logger.info(f"æ‰¹é‡ç´¢å¼•å®Œæˆ: {success_count}/{len(documents)} æˆåŠŸ")
        return success_count

class DistributedStorageDemo:
    """åˆ†å¸ƒå¼å­˜å‚¨æ¼”ç¤º"""
    
    def __init__(self):
        self.cluster = MockElasticsearchCluster()
        self.sample_data = self._generate_sample_data()
    
    def _generate_sample_data(self) -> List[Dict[str, Any]]:
        """ç”Ÿæˆç¤ºä¾‹æˆ¿æºæ•°æ®"""
        cities = ['å¹¿å·', 'æ·±åœ³', 'ä¸Šæµ·', 'åŒ—äº¬']
        districts = ['å¤©æ²³åŒº', 'è¶Šç§€åŒº', 'æµ·ç åŒº', 'ç•ªç¦ºåŒº']
        rental_types = ['æ•´ç§Ÿ', 'åˆç§Ÿ', 'å•é—´']
        
        data = []
        for i in range(50):
            doc = {
                'id': f'house_{i+1}',
                'title': f'ç²¾è£…ä¿®{random.choice(rental_types)}æˆ¿æº{i+1}',
                'city': random.choice(cities),
                'district': random.choice(districts),
                'rental_type': random.choice(rental_types),
                'price': random.randint(2000, 8000),
                'area': round(random.uniform(30, 120), 1),
                'description': f'ä½äº{random.choice(cities)}{random.choice(districts)}çš„ä¼˜è´¨æˆ¿æº',
                'created_at': datetime.now().isoformat()
            }
            data.append(doc)
        
        return data
    
    def demo_data_distribution(self):
        """æ¼”ç¤ºæ•°æ®åˆ†å¸ƒ"""
        logger.info("ğŸš€ å¼€å§‹åˆ†å¸ƒå¼å­˜å‚¨æ¼”ç¤º")
        logger.info("=" * 60)
        
        # æ‰¹é‡ç´¢å¼•æ•°æ®
        self.cluster.bulk_index('house_data', self.sample_data)
        
        # æ˜¾ç¤ºé›†ç¾¤çŠ¶æ€
        health = self.cluster.get_cluster_health()
        logger.info("ğŸ“Š é›†ç¾¤å¥åº·çŠ¶æ€:")
        logger.info(f"  é›†ç¾¤åç§°: {health['cluster_name']}")
        logger.info(f"  èŠ‚ç‚¹æ•°é‡: {health['number_of_nodes']}")
        logger.info(f"  ä¸»åˆ†ç‰‡æ•°: {health['active_primary_shards']}")
        logger.info(f"  æ€»åˆ†ç‰‡æ•°: {health['active_shards']}")
        logger.info(f"  æ€»æ–‡æ¡£æ•°: {health['total_documents']}")
        
        # æ˜¾ç¤ºå„èŠ‚ç‚¹æ•°æ®åˆ†å¸ƒ
        logger.info("\nğŸ“ˆ æ•°æ®åˆ†å¸ƒæƒ…å†µ:")
        for node_stats in health['nodes']:
            logger.info(f"  èŠ‚ç‚¹ {node_stats['node_id']} (ç«¯å£{node_stats['port']}): {node_stats['total_documents']} ä¸ªæ–‡æ¡£")
    
    def demo_distributed_search(self):
        """æ¼”ç¤ºåˆ†å¸ƒå¼æœç´¢"""
        logger.info("\nğŸ” åˆ†å¸ƒå¼æœç´¢æ¼”ç¤º:")
        logger.info("-" * 40)
        
        search_queries = ['å¹¿å·', 'æ•´ç§Ÿ', 'å¤©æ²³åŒº', 'ç²¾è£…ä¿®']
        
        for query in search_queries:
            result = self.cluster.search('house_data', query)
            logger.info(f"æœç´¢ '{query}':")
            logger.info(f"  è€—æ—¶: {result['took']}ms")
            logger.info(f"  å‘½ä¸­: {result['hits']['total']} ä¸ªç»“æœ")
            logger.info(f"  æœç´¢èŠ‚ç‚¹: {result['cluster_info']['nodes_searched']} ä¸ª")
            logger.info(f"  æœç´¢åˆ†ç‰‡: {result['cluster_info']['shards_searched']} ä¸ª")
            
            # æ˜¾ç¤ºå‰3ä¸ªç»“æœ
            for i, hit in enumerate(result['hits']['hits'][:3]):
                source = hit['source']
                logger.info(f"    ç»“æœ{i+1}: {source['title']} - {source['city']}{source['district']} - Â¥{source['price']}")
            
            if result['hits']['total'] > 3:
                logger.info(f"    ... è¿˜æœ‰ {result['hits']['total'] - 3} ä¸ªç»“æœ")
            logger.info("")
    
    def demo_fault_tolerance(self):
        """æ¼”ç¤ºå®¹é”™èƒ½åŠ›"""
        logger.info("ğŸ›¡ï¸ å®¹é”™èƒ½åŠ›æ¼”ç¤º:")
        logger.info("-" * 40)
        
        # æ¨¡æ‹ŸèŠ‚ç‚¹æ•…éšœ
        failed_node = self.cluster.nodes[1]
        failed_node.status = "red"
        logger.info(f"æ¨¡æ‹ŸèŠ‚ç‚¹ {failed_node.node_id} æ•…éšœ...")
        
        # æœç´¢ä»ç„¶å¯ä»¥å·¥ä½œï¼ˆå› ä¸ºæœ‰å‰¯æœ¬ï¼‰
        result = self.cluster.search('house_data', 'å¹¿å·')
        logger.info(f"èŠ‚ç‚¹æ•…éšœåæœç´¢ 'å¹¿å·': ä»ç„¶æ‰¾åˆ° {result['hits']['total']} ä¸ªç»“æœ")
        logger.info("âœ… å‰¯æœ¬åˆ†ç‰‡ä¿è¯äº†æ•°æ®å¯ç”¨æ€§")
        
        # æ¢å¤èŠ‚ç‚¹
        failed_node.status = "green"
        logger.info(f"èŠ‚ç‚¹ {failed_node.node_id} å·²æ¢å¤")
    
    def demo_performance_analysis(self):
        """æ¼”ç¤ºæ€§èƒ½åˆ†æ"""
        logger.info("\nâš¡ æ€§èƒ½åˆ†ææ¼”ç¤º:")
        logger.info("-" * 40)
        
        # æµ‹è¯•ä¸åŒæŸ¥è¯¢çš„æ€§èƒ½
        queries = [
            ('å•ä¸ªå…³é”®è¯', 'å¹¿å·'),
            ('å¤šä¸ªå…³é”®è¯', 'å¹¿å· æ•´ç§Ÿ'),
            ('ä»·æ ¼èŒƒå›´', '3000'),
            ('åŒºåŸŸæœç´¢', 'å¤©æ²³åŒº')
        ]
        
        for query_type, query in queries:
            start_time = time.time()
            result = self.cluster.search('house_data', query)
            end_time = time.time()
            
            logger.info(f"{query_type} ('{query}'):")
            logger.info(f"  æœç´¢è€—æ—¶: {result['took']}ms")
            logger.info(f"  æ€»è€—æ—¶: {(end_time - start_time) * 1000:.1f}ms")
            logger.info(f"  ç»“æœæ•°é‡: {result['hits']['total']}")
            logger.info(f"  å¹³å‡æ¯ç»“æœ: {result['took'] / max(1, result['hits']['total']):.2f}ms")
            logger.info("")
    
    def run_complete_demo(self):
        """è¿è¡Œå®Œæ•´æ¼”ç¤º"""
        try:
            self.demo_data_distribution()
            self.demo_distributed_search()
            self.demo_fault_tolerance()
            self.demo_performance_analysis()
            
            logger.info("ğŸ‰ åˆ†å¸ƒå¼å­˜å‚¨ç³»ç»Ÿæ¼”ç¤ºå®Œæˆ!")
            logger.info("=" * 60)
            logger.info("ğŸ“‹ æ¼”ç¤ºæ€»ç»“:")
            logger.info("âœ… æ•°æ®è‡ªåŠ¨åˆ†ç‰‡åˆ°3ä¸ªèŠ‚ç‚¹")
            logger.info("âœ… å‰¯æœ¬åˆ†ç‰‡ä¿è¯é«˜å¯ç”¨æ€§")
            logger.info("âœ… åˆ†å¸ƒå¼æœç´¢æ¯«ç§’çº§å“åº”")
            logger.info("âœ… èŠ‚ç‚¹æ•…éšœè‡ªåŠ¨å®¹é”™")
            logger.info("âœ… æ”¯æŒå¤æ‚æŸ¥è¯¢å’Œèšåˆ")
            
        except Exception as e:
            logger.error(f"æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")

def main():
    """ä¸»å‡½æ•°"""
    demo = DistributedStorageDemo()
    demo.run_complete_demo()

if __name__ == '__main__':
    main()
