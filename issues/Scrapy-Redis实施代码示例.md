# Scrapy-Redisåˆ†å¸ƒå¼çˆ¬è™«å®æ–½ä»£ç ç¤ºä¾‹

## ğŸ“‹ ä»£ç æ”¹é€ å¯¹æ¯”

### 1. çˆ¬è™«ç±»æ”¹é€ 

**ç°æœ‰ä»£ç  (scrapy_spider/house_spider/spiders/lianjia_spider.py)**:
```python
import scrapy
from house_spider.items import HouseItem

class LianjiaSpider(scrapy.Spider):
    """é“¾å®¶ç§Ÿæˆ¿çˆ¬è™« - å•æœºç‰ˆ"""
    
    name = 'lianjia'
    allowed_domains = ['gz.lianjia.com']
    
    def __init__(self, pages=5, *args, **kwargs):
        super(LianjiaSpider, self).__init__(*args, **kwargs)
        self.pages = int(pages)
        
        # ç”Ÿæˆèµ·å§‹URL
        self.start_urls = [
            f'https://gz.lianjia.com/zufang/pg{i}/#contentList' 
            for i in range(1, self.pages + 1)
        ]
```

**åˆ†å¸ƒå¼æ”¹é€ å**:
```python
import scrapy
from scrapy_redis.spiders import RedisSpider
from house_spider.items import HouseItem

class LianjiaSpider(RedisSpider):
    """é“¾å®¶ç§Ÿæˆ¿çˆ¬è™« - åˆ†å¸ƒå¼ç‰ˆ"""
    
    name = 'lianjia'
    allowed_domains = ['gz.lianjia.com']
    redis_key = 'lianjia:start_urls'
    
    def __init__(self, *args, **kwargs):
        super(LianjiaSpider, self).__init__(*args, **kwargs)
        
    def make_request_from_data(self, data):
        """ä»Redisæ•°æ®åˆ›å»ºè¯·æ±‚"""
        url = data.decode('utf-8')
        return scrapy.Request(url=url, callback=self.parse)
    
    def parse(self, response):
        """è§£ææˆ¿æºåˆ—è¡¨é¡µ - ä¿æŒä¸å˜"""
        # ç°æœ‰è§£æé€»è¾‘ä¿æŒä¸å˜
        ...
        
        # æ–°å¢: å‘ç°æ–°URLæ—¶æ¨é€åˆ°Redis
        for page in range(1, 100):  # åŠ¨æ€å‘ç°æ›´å¤šé¡µé¢
            next_url = f'https://gz.lianjia.com/zufang/pg{page}/#contentList'
            yield scrapy.Request(url=next_url, callback=self.parse)
```

### 2. é…ç½®æ–‡ä»¶æ”¹é€ 

**ç°æœ‰é…ç½® (scrapy_spider/house_spider/settings.py)**:
```python
# ç°æœ‰é…ç½®
CONCURRENT_REQUESTS = 4
CONCURRENT_REQUESTS_PER_DOMAIN = 2
DOWNLOAD_DELAY = 2

# å»é‡å™¨
DUPEFILTER_CLASS = 'scrapy.dupefilters.RFPDupeFilter'

# æ•°æ®ç®¡é“
ITEM_PIPELINES = {
    'house_spider.pipelines.ValidationPipeline': 200,
    'house_spider.pipelines.DuplicatesPipeline': 300,
    'house_spider.pipelines.MySQLPipeline': 400,
}
```

**åˆ†å¸ƒå¼é…ç½®**:
```python
# Redisè¿æ¥é…ç½®
REDIS_HOST = 'localhost'
REDIS_PORT = 6379
REDIS_DB = 0
REDIS_PASSWORD = None
REDIS_PARAMS = {
    'socket_timeout': 30,
    'socket_connect_timeout': 30,
    'retry_on_timeout': True,
    'health_check_interval': 30,
}

# åˆ†å¸ƒå¼è°ƒåº¦å™¨
SCHEDULER = "scrapy_redis.scheduler.Scheduler"
SCHEDULER_PERSIST = True
SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.PriorityQueue'
SCHEDULER_DUPEFILTER_KEY = '%(spider)s:dupefilter'
SCHEDULER_DUPEFILTER_CLASS = 'scrapy_redis.dupefilter.RFPDupeFilter'

# åˆ†å¸ƒå¼å»é‡å™¨
DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
DUPEFILTER_DEBUG = True

# åˆ†å¸ƒå¼æ•°æ®ç®¡é“
ITEM_PIPELINES = {
    'house_spider.pipelines.ValidationPipeline': 200,
    'scrapy_redis.pipelines.RedisPipeline': 300,  # æ–°å¢Redisç®¡é“
    'house_spider.pipelines.MySQLPipeline': 400,
    'house_spider.pipelines.StatisticsPipeline': 500,
}

# æ€§èƒ½ä¼˜åŒ–é…ç½®
CONCURRENT_REQUESTS = 16
CONCURRENT_REQUESTS_PER_DOMAIN = 8
DOWNLOAD_DELAY = 1
RANDOMIZE_DOWNLOAD_DELAY = 0.5

# åˆ†å¸ƒå¼ç‰¹æœ‰é…ç½®
REDIS_START_URLS_AS_SET = True  # ä½¿ç”¨Setå­˜å‚¨èµ·å§‹URL
REDIS_START_URLS_KEY = '%(name)s:start_urls'
```

### 3. Dockerå®¹å™¨åŒ–é…ç½®

**docker-compose.yml**:
```yaml
version: '3.8'

services:
  # RedisæœåŠ¡
  redis:
    image: redis:6.2-alpine
    container_name: scrapy_redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes
    restart: unless-stopped
    
  # Rediså“¨å…µ (é«˜å¯ç”¨)
  redis-sentinel:
    image: redis:6.2-alpine
    container_name: scrapy_redis_sentinel
    ports:
      - "26379:26379"
    depends_on:
      - redis
    volumes:
      - ./sentinel.conf:/usr/local/etc/redis/sentinel.conf
    command: redis-sentinel /usr/local/etc/redis/sentinel.conf
    restart: unless-stopped

  # çˆ¬è™«èŠ‚ç‚¹1
  spider-node-1:
    build: .
    container_name: scrapy_spider_1
    depends_on:
      - redis
      - mysql
      - mongodb
    environment:
      - REDIS_HOST=redis
      - MYSQL_HOST=mysql
      - MONGODB_HOST=mongodb
    volumes:
      - ./scrapy_spider:/app
    command: scrapy crawl lianjia
    restart: unless-stopped

  # çˆ¬è™«èŠ‚ç‚¹2
  spider-node-2:
    build: .
    container_name: scrapy_spider_2
    depends_on:
      - redis
      - mysql
      - mongodb
    environment:
      - REDIS_HOST=redis
      - MYSQL_HOST=mysql
      - MONGODB_HOST=mongodb
    volumes:
      - ./scrapy_spider:/app
    command: scrapy crawl lianjia
    restart: unless-stopped

  # çˆ¬è™«èŠ‚ç‚¹3
  spider-node-3:
    build: .
    container_name: scrapy_spider_3
    depends_on:
      - redis
      - mysql
      - mongodb
    environment:
      - REDIS_HOST=redis
      - MYSQL_HOST=mysql
      - MONGODB_HOST=mongodb
    volumes:
      - ./scrapy_spider:/app
    command: scrapy crawl lianjia
    restart: unless-stopped

  # MySQLæ•°æ®åº“
  mysql:
    image: mysql:8.0
    container_name: scrapy_mysql
    environment:
      MYSQL_ROOT_PASSWORD: 123456
      MYSQL_DATABASE: guangzhou_house
    ports:
      - "3306:3306"
    volumes:
      - mysql_data:/var/lib/mysql
    restart: unless-stopped

  # MongoDBæ•°æ®åº“
  mongodb:
    image: mongo:5.0
    container_name: scrapy_mongodb
    ports:
      - "27017:27017"
    volumes:
      - mongodb_data:/data/db
    restart: unless-stopped

  # ç›‘æ§æœåŠ¡
  prometheus:
    image: prom/prometheus
    container_name: scrapy_prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
    restart: unless-stopped

  grafana:
    image: grafana/grafana
    container_name: scrapy_grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana_data:/var/lib/grafana
    restart: unless-stopped

volumes:
  redis_data:
  mysql_data:
  mongodb_data:
  grafana_data:
```

### 4. å¯åŠ¨è„šæœ¬æ”¹é€ 

**åˆ†å¸ƒå¼å¯åŠ¨è„šæœ¬ (start_distributed_spider.py)**:
```python
#!/usr/bin/env python3
import redis
import argparse
import logging
from scrapy.utils.project import get_project_settings

def setup_redis_urls(redis_client, spider_name, pages=10):
    """è®¾ç½®Redisèµ·å§‹URL"""
    key = f"{spider_name}:start_urls"
    
    # æ¸…ç©ºç°æœ‰URL
    redis_client.delete(key)
    
    # æ·»åŠ èµ·å§‹URL
    urls = [
        f'https://gz.lianjia.com/zufang/pg{i}/#contentList' 
        for i in range(1, pages + 1)
    ]
    
    for url in urls:
        redis_client.sadd(key, url)
    
    print(f"âœ… å·²æ·»åŠ  {len(urls)} ä¸ªèµ·å§‹URLåˆ°Redis")
    return len(urls)

def check_cluster_status(redis_client):
    """æ£€æŸ¥é›†ç¾¤çŠ¶æ€"""
    try:
        # æ£€æŸ¥Redisè¿æ¥
        redis_client.ping()
        print("âœ… Redisè¿æ¥æ­£å¸¸")
        
        # æ£€æŸ¥é˜Ÿåˆ—çŠ¶æ€
        keys = redis_client.keys("*")
        print(f"ğŸ“Š Redisä¸­å…±æœ‰ {len(keys)} ä¸ªé”®")
        
        return True
    except Exception as e:
        print(f"âŒ é›†ç¾¤çŠ¶æ€æ£€æŸ¥å¤±è´¥: {e}")
        return False

def main():
    parser = argparse.ArgumentParser(description='åˆ†å¸ƒå¼çˆ¬è™«å¯åŠ¨å™¨')
    parser.add_argument('-p', '--pages', type=int, default=10, help='çˆ¬å–é¡µæ•°')
    parser.add_argument('-s', '--spider', default='lianjia', help='çˆ¬è™«åç§°')
    parser.add_argument('--setup', action='store_true', help='è®¾ç½®èµ·å§‹URL')
    parser.add_argument('--status', action='store_true', help='æ£€æŸ¥é›†ç¾¤çŠ¶æ€')
    parser.add_argument('--clear', action='store_true', help='æ¸…ç©ºRedisæ•°æ®')
    
    args = parser.parse_args()
    
    # è¿æ¥Redis
    settings = get_project_settings()
    redis_client = redis.Redis(
        host=settings.get('REDIS_HOST', 'localhost'),
        port=settings.get('REDIS_PORT', 6379),
        db=settings.get('REDIS_DB', 0),
        decode_responses=True
    )
    
    if args.status:
        check_cluster_status(redis_client)
        return
    
    if args.clear:
        redis_client.flushdb()
        print("âœ… Redisæ•°æ®å·²æ¸…ç©º")
        return
    
    if args.setup:
        setup_redis_urls(redis_client, args.spider, args.pages)
        return
    
    print("åˆ†å¸ƒå¼çˆ¬è™«é›†ç¾¤ç®¡ç†å™¨")
    print("ä½¿ç”¨ --help æŸ¥çœ‹å¯ç”¨é€‰é¡¹")

if __name__ == '__main__':
    main()
```

### 5. ç›‘æ§é…ç½®

**Prometheusé…ç½® (monitoring/prometheus.yml)**:
```yaml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'scrapy-spiders'
    static_configs:
      - targets: ['spider-node-1:8000', 'spider-node-2:8000', 'spider-node-3:8000']
    
  - job_name: 'redis'
    static_configs:
      - targets: ['redis:6379']
    
  - job_name: 'mysql'
    static_configs:
      - targets: ['mysql:3306']
```

### 6. æ–°å¢ä¾èµ–

**requirements_distributed.txt**:
```
# ç°æœ‰ä¾èµ–
Django>=4.0.0
pandas>=1.3.0
numpy>=1.21.0
matplotlib>=3.5.0
scikit-learn>=1.0.0
wordcloud>=1.8.0
PyMySQL>=1.0.0
requests>=2.25.0
lxml>=4.6.0
Pillow>=8.0.0
Scrapy>=2.8.0

# åˆ†å¸ƒå¼æ–°å¢ä¾èµ–
scrapy-redis>=0.7.0
redis>=4.5.0
docker>=6.0.0
docker-compose>=2.0.0

# ç›‘æ§ç›¸å…³
prometheus-client>=0.15.0
grafana-api>=1.0.3

# å®¹å™¨åŒ–ç›¸å…³
gunicorn>=20.1.0
supervisor>=4.2.0
```

---

## ğŸš€ éƒ¨ç½²æµç¨‹

### 1. ç¯å¢ƒå‡†å¤‡
```bash
# å®‰è£…ä¾èµ–
pip install -r requirements_distributed.txt

# å¯åŠ¨Redis
docker-compose up -d redis

# éªŒè¯Redisè¿æ¥
python -c "import redis; r=redis.Redis(); print(r.ping())"
```

### 2. é…ç½®éƒ¨ç½²
```bash
# è®¾ç½®èµ·å§‹URL
python start_distributed_spider.py --setup --pages 50

# æ£€æŸ¥é›†ç¾¤çŠ¶æ€
python start_distributed_spider.py --status
```

### 3. å¯åŠ¨é›†ç¾¤
```bash
# å¯åŠ¨å®Œæ•´é›†ç¾¤
docker-compose up -d

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f spider-node-1
```

### 4. ç›‘æ§éªŒè¯
```bash
# è®¿é—®ç›‘æ§é¢æ¿
# Prometheus: http://localhost:9090
# Grafana: http://localhost:3000 (admin/admin)

# æ£€æŸ¥çˆ¬è™«çŠ¶æ€
docker-compose ps
```

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”æµ‹è¯•

### æµ‹è¯•è„šæœ¬
```python
import time
import redis
import pymysql
from datetime import datetime

def performance_test():
    """æ€§èƒ½å¯¹æ¯”æµ‹è¯•"""
    
    # è¿æ¥Redis
    r = redis.Redis(host='localhost', port=6379, db=0)
    
    # è¿æ¥MySQL
    conn = pymysql.connect(
        host='localhost',
        user='root',
        password='123456',
        database='guangzhou_house'
    )
    
    start_time = datetime.now()
    
    # ç›‘æ§æŒ‡æ ‡
    while True:
        # Redisé˜Ÿåˆ—é•¿åº¦
        queue_len = r.llen('lianjia:requests')
        
        # å·²å¤„ç†è¯·æ±‚æ•°
        processed = r.get('lianjia:processed') or 0
        
        # æ•°æ®åº“è®°å½•æ•°
        cursor = conn.cursor()
        cursor.execute("SELECT COUNT(*) FROM house_scrapy WHERE crawl_time > %s", [start_time])
        db_count = cursor.fetchone()[0]
        
        print(f"é˜Ÿåˆ—é•¿åº¦: {queue_len}, å·²å¤„ç†: {processed}, æ•°æ®åº“: {db_count}")
        time.sleep(10)

if __name__ == '__main__':
    performance_test()
```

è¿™ä¸ªè¯¦ç»†çš„ä»£ç ç¤ºä¾‹å±•ç¤ºäº†ä»å•æœºScrapyåˆ°åˆ†å¸ƒå¼Scrapy-Redisçš„å®Œæ•´æ”¹é€ è¿‡ç¨‹ï¼ŒåŒ…æ‹¬æ‰€æœ‰å¿…è¦çš„é…ç½®å’Œéƒ¨ç½²è„šæœ¬ã€‚
