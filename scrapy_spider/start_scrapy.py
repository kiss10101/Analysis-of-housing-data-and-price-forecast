#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Scrapyçˆ¬è™«å¯åŠ¨å™¨ - ç”Ÿäº§ç‰ˆæœ¬
æˆ¿æºæ•°æ®åˆ†æç³»ç»Ÿ
"""

import os
import sys
import argparse
import logging
import subprocess
from datetime import datetime

def setup_logging(log_level='INFO'):
    """è®¾ç½®æ—¥å¿—"""
    current_dir = os.path.dirname(os.path.abspath(__file__))
    log_dir = os.path.join(current_dir, 'logs')
    
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)
        
    log_file = os.path.join(log_dir, f'scrapy_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')
    
    logging.basicConfig(
        level=getattr(logging, log_level.upper()),
        format='%(asctime)s [%(name)s] %(levelname)s: %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    return log_file

def run_scrapy_spider(pages=5, spider_name='lianjia', log_level='INFO'):
    """è¿è¡ŒScrapyçˆ¬è™«"""
    
    # è®¾ç½®æ—¥å¿—
    log_file = setup_logging(log_level)
    logger = logging.getLogger(__name__)
    
    logger.info("=" * 60)
    logger.info("æˆ¿æºæ•°æ®åˆ†æç³»ç»Ÿ - Scrapyçˆ¬è™«å¯åŠ¨")
    logger.info("=" * 60)
    logger.info(f"çˆ¬è™«åç§°: {spider_name}")
    logger.info(f"çˆ¬å–é¡µæ•°: {pages}")
    logger.info(f"æ—¥å¿—çº§åˆ«: {log_level}")
    logger.info(f"æ—¥å¿—æ–‡ä»¶: {log_file}")
    logger.info(f"å¯åŠ¨æ—¶é—´: {datetime.now()}")
    
    try:
        # è·å–å½“å‰ç›®å½•
        current_dir = os.path.dirname(os.path.abspath(__file__))
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        env = os.environ.copy()
        env['PYTHONPATH'] = current_dir
        
        # æ„å»ºå‘½ä»¤
        cmd = [
            sys.executable, '-m', 'scrapy', 'crawl', spider_name,
            '-a', f'pages={pages}',
            '-s', f'LOG_LEVEL={log_level}',
            '-s', f'LOG_FILE={log_file}'
        ]
        
        logger.info(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
        logger.info("å¼€å§‹çˆ¬å–æ•°æ®...")
        
        # è¿è¡Œå‘½ä»¤
        process = subprocess.Popen(
            cmd,
            cwd=current_dir,
            env=env,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            bufsize=1,
            universal_newlines=True
        )
        
        # å®æ—¶è¾“å‡ºæ—¥å¿—
        for line in iter(process.stdout.readline, ''):
            if line.strip():
                print(line.strip())
                
        process.wait()
        
        if process.returncode == 0:
            logger.info("âœ… çˆ¬å–å®Œæˆ!")
            return True
        else:
            logger.error(f"âŒ çˆ¬å–å¤±è´¥ï¼Œè¿”å›ç : {process.returncode}")
            return False
            
    except Exception as e:
        logger.error(f"âŒ çˆ¬è™«è¿è¡Œå¼‚å¸¸: {e}")
        return False

def check_environment():
    """æ£€æŸ¥è¿è¡Œç¯å¢ƒ"""
    print("æ£€æŸ¥è¿è¡Œç¯å¢ƒ...")
    
    try:
        # æ£€æŸ¥Pythonç‰ˆæœ¬
        python_version = sys.version_info
        print(f"âœ… Pythonç‰ˆæœ¬: {python_version.major}.{python_version.minor}.{python_version.micro}")
        
        # æ£€æŸ¥Scrapy
        import scrapy
        print(f"âœ… Scrapyç‰ˆæœ¬: {scrapy.__version__}")
        
        # æ£€æŸ¥æ•°æ®åº“è¿æ¥
        import pymysql
        conn = pymysql.connect(
            host='localhost',
            user='root',
            password='123456',
            database='guangzhou_house'
        )
        conn.close()
        print("âœ… MySQLè¿æ¥æ­£å¸¸")
        
        # æ£€æŸ¥é¡¹ç›®æ¨¡å—
        current_dir = os.path.dirname(os.path.abspath(__file__))
        sys.path.insert(0, current_dir)
        
        from house_spider.spiders.lianjia_spider import LianjiaSpider
        print("âœ… çˆ¬è™«æ¨¡å—æ­£å¸¸")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç¯å¢ƒæ£€æŸ¥å¤±è´¥: {e}")
        return False

def show_statistics():
    """æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯"""
    print("\n" + "=" * 50)
    print("æ•°æ®ç»Ÿè®¡")
    print("=" * 50)
    
    try:
        import pymysql
        
        conn = pymysql.connect(
            host='localhost',
            user='root',
            password='123456',
            database='guangzhou_house',
            charset='utf8mb4'
        )
        
        cursor = conn.cursor()
        
        # åŸå§‹è¡¨ç»Ÿè®¡
        cursor.execute("SELECT COUNT(*) FROM House")
        original_count = cursor.fetchone()[0]
        print(f"åŸå§‹Houseè¡¨è®°å½•æ•°: {original_count}")
        
        # Scrapyè¡¨ç»Ÿè®¡
        cursor.execute("SHOW TABLES LIKE 'House_scrapy'")
        if cursor.fetchone():
            cursor.execute("SELECT COUNT(*) FROM House_scrapy")
            scrapy_count = cursor.fetchone()[0]
            print(f"Scrapyå¤‡ä»½è¡¨è®°å½•æ•°: {scrapy_count}")
            
            if scrapy_count > 0:
                # æœ€æ–°è®°å½•
                cursor.execute("""
                    SELECT crawl_time, COUNT(*) as count
                    FROM House_scrapy 
                    GROUP BY DATE(crawl_time)
                    ORDER BY crawl_time DESC
                    LIMIT 5
                """)
                daily_stats = cursor.fetchall()
                
                print("\næœ€è¿‘çˆ¬å–ç»Ÿè®¡:")
                for stat in daily_stats:
                    print(f"  {stat[0]}: {stat[1]} æ¡")
        else:
            print("Scrapyå¤‡ä»½è¡¨: ä¸å­˜åœ¨")
        
        cursor.close()
        conn.close()
        
    except Exception as e:
        print(f"ç»Ÿè®¡ä¿¡æ¯è·å–å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æˆ¿æºæ•°æ®åˆ†æç³»ç»Ÿ - Scrapyçˆ¬è™«å¯åŠ¨å™¨')
    parser.add_argument('-p', '--pages', type=int, default=5, help='çˆ¬å–é¡µæ•° (é»˜è®¤: 5)')
    parser.add_argument('-s', '--spider', default='lianjia', help='çˆ¬è™«åç§° (é»˜è®¤: lianjia)')
    parser.add_argument('-l', '--log-level', default='INFO', 
                       choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
                       help='æ—¥å¿—çº§åˆ« (é»˜è®¤: INFO)')
    parser.add_argument('--check', action='store_true', help='æ£€æŸ¥ç¯å¢ƒ')
    parser.add_argument('--stats', action='store_true', help='æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯')
    parser.add_argument('--test', action='store_true', help='æµ‹è¯•æ¨¡å¼ (åªçˆ¬å–1é¡µ)')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥ç¯å¢ƒ
    if args.check:
        check_environment()
        return
    
    # æ˜¾ç¤ºç»Ÿè®¡
    if args.stats:
        show_statistics()
        return
    
    # æµ‹è¯•æ¨¡å¼
    if args.test:
        args.pages = 1
        args.log_level = 'DEBUG'
        print("ğŸ§ª æµ‹è¯•æ¨¡å¼: åªçˆ¬å–1é¡µæ•°æ®")
    
    print("æˆ¿æºæ•°æ®åˆ†æç³»ç»Ÿ - Scrapyçˆ¬è™«å¯åŠ¨å™¨")
    print(f"å¯åŠ¨æ—¶é—´: {datetime.now()}")
    
    # æ£€æŸ¥ç¯å¢ƒ
    if not check_environment():
        print("âŒ ç¯å¢ƒæ£€æŸ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
        sys.exit(1)
    
    # è¿è¡Œçˆ¬è™«
    success = run_scrapy_spider(
        pages=args.pages,
        spider_name=args.spider,
        log_level=args.log_level
    )
    
    if success:
        print("\nğŸ‰ çˆ¬å–ä»»åŠ¡å®Œæˆ!")
        show_statistics()
    else:
        print("\nâŒ çˆ¬å–ä»»åŠ¡å¤±è´¥!")
        sys.exit(1)

if __name__ == '__main__':
    main()
