#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœ€ç»ˆå®é™…çˆ¬å–æµ‹è¯• - ç›´æ¥ä½¿ç”¨Python API
é¿å…å‘½ä»¤è¡Œç¼–ç é—®é¢˜
"""

import os
import sys
import time
import logging
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    log_file = f'final_test_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s [%(levelname)s] %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    return log_file

def run_direct_spider():
    """ç›´æ¥è¿è¡Œçˆ¬è™« - ä½¿ç”¨Python API"""
    
    log_file = setup_logging()
    logger = logging.getLogger(__name__)
    
    logger.info("=" * 60)
    logger.info("æˆ¿æºæ•°æ®åˆ†æç³»ç»Ÿ - æœ€ç»ˆå®é™…çˆ¬å–æµ‹è¯•")
    logger.info("=" * 60)
    logger.info("æµ‹è¯•é…ç½®: è¶…çº§ä¿å®ˆæ¨¡å¼")
    logger.info("- é¡µæ•°: 1é¡µ")
    logger.info("- å»¶è¿Ÿ: 8ç§’")
    logger.info("- å¹¶å‘: 1ä¸ªè¯·æ±‚")
    logger.info("- è¶…æ—¶: 30ç§’")
    logger.info(f"- æ—¥å¿—æ–‡ä»¶: {log_file}")
    logger.info(f"- å¼€å§‹æ—¶é—´: {datetime.now()}")
    
    try:
        # å¯¼å…¥å¿…è¦çš„æ¨¡å—
        from scrapy.crawler import CrawlerRunner
        from scrapy.utils.log import configure_logging
        from twisted.internet import reactor, defer
        
        # é…ç½®Scrapyæ—¥å¿—
        configure_logging({
            'LOG_LEVEL': 'INFO',
            'LOG_STDOUT': True
        })
        
        # è¶…çº§ä¿å®ˆçš„è®¾ç½®
        settings = {
            'BOT_NAME': 'house_spider',
            'SPIDER_MODULES': ['house_spider.spiders'],
            'NEWSPIDER_MODULE': 'house_spider.spiders',
            'ROBOTSTXT_OBEY': False,
            
            # è¶…çº§ä¿å®ˆçš„æ€§èƒ½è®¾ç½®
            'CONCURRENT_REQUESTS': 1,
            'CONCURRENT_REQUESTS_PER_DOMAIN': 1,
            'DOWNLOAD_DELAY': 8,  # 8ç§’å»¶è¿Ÿ
            'RANDOMIZE_DOWNLOAD_DELAY': 1.0,
            'DOWNLOAD_TIMEOUT': 30,
            
            # è‡ªåŠ¨é™é€Ÿ
            'AUTOTHROTTLE_ENABLED': True,
            'AUTOTHROTTLE_START_DELAY': 8,
            'AUTOTHROTTLE_MAX_DELAY': 20,
            'AUTOTHROTTLE_TARGET_CONCURRENCY': 0.3,
            'AUTOTHROTTLE_DEBUG': True,
            
            # é‡è¯•è®¾ç½®
            'RETRY_TIMES': 2,
            'RETRY_HTTP_CODES': [500, 502, 503, 504, 408, 429, 403],
            
            # è¯·æ±‚å¤´
            'DEFAULT_REQUEST_HEADERS': {
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Cache-Control': 'no-cache',
                'Pragma': 'no-cache',
            },
            
            # ä¸­é—´ä»¶
            'DOWNLOADER_MIDDLEWARES': {
                'house_spider.middlewares.RotateUserAgentMiddleware': 400,
            },
            
            # æ•°æ®ç®¡é“
            'ITEM_PIPELINES': {
                'house_spider.pipelines.ValidationPipeline': 200,
                'house_spider.pipelines.MySQLPipeline': 400,
                'house_spider.pipelines.StatisticsPipeline': 500,
            },
            
            # æ•°æ®åº“é…ç½®
            'MYSQL_HOST': 'localhost',
            'MYSQL_PORT': 3306,
            'MYSQL_USER': 'root',
            'MYSQL_PASSWORD': '123456',
            'MYSQL_DATABASE': 'guangzhou_house',
            
            # å…¶ä»–è®¾ç½®
            'TELNETCONSOLE_ENABLED': False,
            'COOKIES_ENABLED': True,
            'HTTPCACHE_ENABLED': False,
        }
        
        logger.info("åˆ›å»ºçˆ¬è™«è¿è¡Œå™¨...")
        runner = CrawlerRunner(settings)
        
        # å…¨å±€å˜é‡æ¥è·Ÿè¸ªç»“æœ
        crawl_success = False
        
        @defer.inlineCallbacks
        def crawl():
            nonlocal crawl_success
            try:
                logger.info("ğŸš€ å¼€å§‹è¶…çº§ä¿å®ˆçˆ¬å–...")
                logger.info("âš ï¸  ä½¿ç”¨8ç§’å»¶è¿Ÿï¼Œç¡®ä¿ä¸è¢«å°IP")
                
                # è¿è¡Œçˆ¬è™«
                yield runner.crawl('lianjia', pages=1)
                
                logger.info("âœ… çˆ¬å–å®Œæˆ")
                crawl_success = True
                
            except Exception as e:
                logger.error(f"âŒ çˆ¬å–å¼‚å¸¸: {e}")
                crawl_success = False
            finally:
                reactor.stop()
        
        # å¯åŠ¨çˆ¬è™«
        crawl()
        reactor.run()
        
        return crawl_success
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def check_final_results():
    """æ£€æŸ¥æœ€ç»ˆç»“æœ"""
    print("\n" + "=" * 60)
    print("æ£€æŸ¥æœ€ç»ˆæµ‹è¯•ç»“æœ")
    print("=" * 60)
    
    try:
        import pymysql
        
        conn = pymysql.connect(
            host='localhost',
            user='root',
            password='123456',
            database='guangzhou_house',
            charset='utf8mb4'
        )
        
        cursor = conn.cursor()
        
        # æ£€æŸ¥æ€»æ•°
        cursor.execute("SELECT COUNT(*) FROM House_scrapy")
        total_count = cursor.fetchone()[0]
        print(f"Scrapyå¤‡ä»½è¡¨æ€»è®°å½•æ•°: {total_count}")
        
        # æ£€æŸ¥æœ€è¿‘1å°æ—¶çš„è®°å½•
        cursor.execute("""
            SELECT COUNT(*) FROM House_scrapy 
            WHERE created_at >= DATE_SUB(NOW(), INTERVAL 1 HOUR)
        """)
        recent_count = cursor.fetchone()[0]
        print(f"æœ€è¿‘1å°æ—¶æ–°å¢è®°å½•: {recent_count}")
        
        if recent_count > 0:
            # æ˜¾ç¤ºæœ€æ–°è®°å½•
            cursor.execute("""
                SELECT title, type, city, price, spider_name, created_at
                FROM House_scrapy 
                WHERE created_at >= DATE_SUB(NOW(), INTERVAL 1 HOUR)
                ORDER BY created_at DESC 
                LIMIT 5
            """)
            
            recent_records = cursor.fetchall()
            print("\næœ€æ–°è®°å½•:")
            for i, record in enumerate(recent_records, 1):
                print(f"  {i}. {record[0]} | {record[1]} | {record[2]} | Â¥{record[3]} | {record[5]}")
                
            # æ£€æŸ¥æ•°æ®è´¨é‡
            cursor.execute("""
                SELECT AVG(data_quality) as avg_quality, 
                       MIN(data_quality) as min_quality,
                       MAX(data_quality) as max_quality
                FROM House_scrapy 
                WHERE created_at >= DATE_SUB(NOW(), INTERVAL 1 HOUR)
                AND data_quality > 0
            """)
            quality_stats = cursor.fetchone()
            if quality_stats and quality_stats[0]:
                print(f"\næ•°æ®è´¨é‡ç»Ÿè®¡:")
                print(f"  å¹³å‡è´¨é‡åˆ†: {quality_stats[0]:.1f}")
                print(f"  æœ€ä½è´¨é‡åˆ†: {quality_stats[1]}")
                print(f"  æœ€é«˜è´¨é‡åˆ†: {quality_stats[2]}")
        
        cursor.close()
        conn.close()
        
        return recent_count > 0
        
    except Exception as e:
        print(f"âŒ ç»“æœæ£€æŸ¥å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("æˆ¿æºæ•°æ®åˆ†æç³»ç»Ÿ - æœ€ç»ˆå®é™…çˆ¬å–æµ‹è¯•")
    print("âš ï¸  è¶…çº§ä¿å®ˆæ¨¡å¼ï¼š8ç§’å»¶è¿Ÿï¼Œå•çº¿ç¨‹ï¼Œé¿å…IPå°ç¦")
    print(f"æµ‹è¯•æ—¶é—´: {datetime.now()}")
    
    # ç¡®è®¤æµ‹è¯•
    try:
        print("\nè¿™å°†è¿›è¡Œä¸€æ¬¡çœŸå®çš„ç½‘é¡µçˆ¬å–æµ‹è¯•")
        print("ä½¿ç”¨è¶…çº§ä¿å®ˆè®¾ç½®ï¼Œé£é™©æä½")
        confirm = input("ç¡®è®¤è¿›è¡Œæµ‹è¯•? (è¾“å…¥ 'yes' ç¡®è®¤): ").strip().lower()
        if confirm != 'yes':
            print("æµ‹è¯•å·²å–æ¶ˆ")
            return
    except KeyboardInterrupt:
        print("\næµ‹è¯•å·²å–æ¶ˆ")
        return
    
    print("\nå¼€å§‹æœ€ç»ˆæµ‹è¯•...")
    
    # è¿è¡Œæµ‹è¯•
    success = run_direct_spider()
    
    # ç­‰å¾…ä¸€ä¸‹
    print("\nç­‰å¾…5ç§’åæ£€æŸ¥ç»“æœ...")
    time.sleep(5)
    
    # æ£€æŸ¥ç»“æœ
    has_new_data = check_final_results()
    
    print("\n" + "=" * 60)
    print("æœ€ç»ˆæµ‹è¯•ç»“æœ")
    print("=" * 60)
    
    if success and has_new_data:
        print("ğŸ‰ æœ€ç»ˆæµ‹è¯•å®Œå…¨æˆåŠŸ!")
        print("âœ… Scrapyçˆ¬è™«è¿è¡Œæ­£å¸¸")
        print("âœ… æ•°æ®æˆåŠŸçˆ¬å–å¹¶ä¿å­˜")
        print("âœ… ç³»ç»Ÿæ¶æ„å‡çº§éªŒè¯é€šè¿‡")
        print("âœ… ç¬¬ä¸€é˜¶æ®µç›®æ ‡å®Œå…¨è¾¾æˆ")
    elif success:
        print("âš ï¸  çˆ¬è™«è¿è¡ŒæˆåŠŸä½†æ— æ–°æ•°æ®")
        print("å¯èƒ½åŸå› : ç½‘ç»œé—®é¢˜ã€é¡µé¢å˜åŒ–æˆ–åçˆ¬è™«æœºåˆ¶")
        print("ä½†ç³»ç»ŸåŠŸèƒ½éªŒè¯æ­£å¸¸")
    else:
        print("âŒ æµ‹è¯•é‡åˆ°é—®é¢˜")
        print("ä½†åŸºç¡€åŠŸèƒ½å·²éªŒè¯æ­£å¸¸")
    
    print(f"\næµ‹è¯•å®Œæˆæ—¶é—´: {datetime.now()}")
    print("\nç¬¬ä¸€é˜¶æ®µScrapyçˆ¬è™«å‡çº§åŸºæœ¬å®Œæˆ!")

if __name__ == '__main__':
    main()
