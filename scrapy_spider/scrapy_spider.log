2025-06-22 10:59:19 [scrapy.utils.log] INFO: Scrapy 2.13.2 started (bot: house_spider)
2025-06-22 10:59:19 [scrapy.utils.log] INFO: Versions:
{'lxml': '5.4.0',
 'libxml2': '2.11.9',
 'cssselect': '1.3.0',
 'parsel': '1.10.0',
 'w3lib': '2.3.1',
 'Twisted': '25.5.0',
 'Python': '3.11.3 (tags/v3.11.3:f3909b8, Apr  4 2023, 23:49:59) [MSC v.1934 '
           '64 bit (AMD64)]',
 'pyOpenSSL': '25.1.0 (OpenSSL 3.5.0 8 Apr 2025)',
 'cryptography': '45.0.4',
 'Platform': 'Windows-10-10.0.22631-SP0'}
2025-06-22 10:59:19 [lianjia] INFO: 初始化爬虫，爬取页数: 1, 批次ID: 8732e3bf-55ec-40b5-b608-98a75038dded
2025-06-22 10:59:19 [scrapy.addons] INFO: Enabled addons:
[]
2025-06-22 10:59:19 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.throttle.AutoThrottle']
2025-06-22 10:59:19 [scrapy.crawler] INFO: Overridden settings:
{'AUTOTHROTTLE_DEBUG': 'True',
 'AUTOTHROTTLE_ENABLED': 'True',
 'AUTOTHROTTLE_MAX_DELAY': '20',
 'AUTOTHROTTLE_START_DELAY': '8',
 'AUTOTHROTTLE_TARGET_CONCURRENCY': '0.3',
 'BOT_NAME': 'house_spider',
 'CONCURRENT_REQUESTS': '1',
 'CONCURRENT_REQUESTS_PER_DOMAIN': 2,
 'DOWNLOAD_DELAY': '8',
 'DOWNLOAD_TIMEOUT': '30',
 'LOG_FILE': 'scrapy_spider.log',
 'LOG_LEVEL': 'INFO',
 'MEMUSAGE_LIMIT_MB': 2048,
 'MEMUSAGE_WARNING_MB': 1024,
 'NEWSPIDER_MODULE': 'house_spider.spiders',
 'RANDOMIZE_DOWNLOAD_DELAY': '1.0',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 408, 429],
 'RETRY_TIMES': '2',
 'SPIDER_MODULES': ['house_spider.spiders'],
 'TELNETCONSOLE_ENABLED': False}
2025-06-22 10:59:20 [twisted] CRITICAL: Unhandled error in Deferred:
2025-06-22 10:59:20 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "E:\python\python3.11.3\Lib\site-packages\scrapy\settings\__init__.py", line 170, in getbool
    return bool(int(got))
                ^^^^^^^^
ValueError: invalid literal for int() with base 10: '1.0'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "E:\python\python3.11.3\Lib\site-packages\twisted\internet\defer.py", line 1857, in _inlineCallbacks
    result = context.run(gen.send, result)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "E:\python\python3.11.3\Lib\site-packages\scrapy\crawler.py", line 156, in crawl
    self.engine = self._create_engine()
                  ^^^^^^^^^^^^^^^^^^^^^
  File "E:\python\python3.11.3\Lib\site-packages\scrapy\crawler.py", line 169, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "E:\python\python3.11.3\Lib\site-packages\scrapy\core\engine.py", line 110, in __init__
    self.downloader: Downloader = downloader_cls(crawler)
                                  ^^^^^^^^^^^^^^^^^^^^^^^
  File "E:\python\python3.11.3\Lib\site-packages\scrapy\core\downloader\__init__.py", line 107, in __init__
    self.randomize_delay: bool = self.settings.getbool("RANDOMIZE_DOWNLOAD_DELAY")
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "E:\python\python3.11.3\Lib\site-packages\scrapy\settings\__init__.py", line 176, in getbool
    raise ValueError(
ValueError: Supported values for boolean settings are 0/1, True/False, '0'/'1', 'True'/'False' and 'true'/'false'
2025-06-22 10:59:44 [scrapy.utils.log] INFO: Scrapy 2.13.2 started (bot: house_spider)
2025-06-22 10:59:44 [scrapy.utils.log] INFO: Versions:
{'lxml': '5.4.0',
 'libxml2': '2.11.9',
 'cssselect': '1.3.0',
 'parsel': '1.10.0',
 'w3lib': '2.3.1',
 'Twisted': '25.5.0',
 'Python': '3.11.3 (tags/v3.11.3:f3909b8, Apr  4 2023, 23:49:59) [MSC v.1934 '
           '64 bit (AMD64)]',
 'pyOpenSSL': '25.1.0 (OpenSSL 3.5.0 8 Apr 2025)',
 'cryptography': '45.0.4',
 'Platform': 'Windows-10-10.0.22631-SP0'}
2025-06-22 10:59:44 [lianjia] INFO: 初始化爬虫，爬取页数: 1, 批次ID: 1ed42aac-2d35-441a-a10b-4c300957aa78
2025-06-22 10:59:44 [scrapy.addons] INFO: Enabled addons:
[]
2025-06-22 10:59:44 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.throttle.AutoThrottle']
2025-06-22 10:59:44 [scrapy.crawler] INFO: Overridden settings:
{'AUTOTHROTTLE_DEBUG': 'True',
 'AUTOTHROTTLE_ENABLED': 'True',
 'AUTOTHROTTLE_MAX_DELAY': '20',
 'AUTOTHROTTLE_START_DELAY': '8',
 'AUTOTHROTTLE_TARGET_CONCURRENCY': '0.3',
 'BOT_NAME': 'house_spider',
 'CONCURRENT_REQUESTS': '1',
 'CONCURRENT_REQUESTS_PER_DOMAIN': 2,
 'DOWNLOAD_DELAY': '8',
 'DOWNLOAD_TIMEOUT': '30',
 'LOG_FILE': 'scrapy_spider.log',
 'LOG_LEVEL': 'INFO',
 'MEMUSAGE_LIMIT_MB': 2048,
 'MEMUSAGE_WARNING_MB': 1024,
 'NEWSPIDER_MODULE': 'house_spider.spiders',
 'RANDOMIZE_DOWNLOAD_DELAY': 'True',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 408, 429],
 'RETRY_TIMES': '2',
 'SPIDER_MODULES': ['house_spider.spiders'],
 'TELNETCONSOLE_ENABLED': False}
2025-06-22 10:59:45 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'house_spider.middlewares.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'house_spider.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'house_spider.middlewares.RequestLoggingMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'house_spider.middlewares.AntiSpiderMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-06-22 10:59:45 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.start.StartSpiderMiddleware',
 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-06-22 10:59:45 [scrapy.middleware] INFO: Enabled item pipelines:
['house_spider.pipelines.ValidationPipeline',
 'house_spider.pipelines.DuplicatesPipeline',
 'house_spider.pipelines.MySQLPipeline',
 'house_spider.pipelines.StatisticsPipeline']
2025-06-22 10:59:45 [scrapy.core.engine] INFO: Spider opened
2025-06-22 10:59:45 [lianjia] INFO: MySQL连接成功: localhost:3306/guangzhou_house
2025-06-22 10:59:45 [lianjia] INFO: 备份表 House_scrapy 创建成功
2025-06-22 10:59:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-06-22 10:59:55 [scrapy.extensions.throttle] INFO: slot: gz.lianjia.com | conc: 1 | delay: 8000 ms (+0) | latency: 1031 ms | size: 20482 bytes
2025-06-22 10:59:57 [lianjia] INFO: 正在解析页面: https://gz.lianjia.com/zufang/pg1/
2025-06-22 10:59:57 [lianjia] INFO: 找到 30 个房源
2025-06-22 10:59:57 [scrapy.core.engine] INFO: Closing spider (finished)
2025-06-22 10:59:57 [lianjia] INFO: 爬取完成 - 总计: 30 条数据，错误: 0 条
2025-06-22 10:59:57 [lianjia] INFO: MySQL连接已关闭
2025-06-22 10:59:57 [scrapy.extensions.feedexport] INFO: Stored json feed (30 items) in: output/houses_2025-06-22T02-59-45+00-00.json
2025-06-22 10:59:57 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 314,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 21080,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 12.409321,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 6, 22, 2, 59, 57, 664033, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 144159,
 'httpcompression/response_count': 1,
 'item_scraped_count': 30,
 'items_per_minute': 150.0,
 'log_count/INFO': 16,
 'response_received_count': 1,
 'responses_per_minute': 5.0,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2025, 6, 22, 2, 59, 45, 254712, tzinfo=datetime.timezone.utc)}
2025-06-22 10:59:57 [scrapy.core.engine] INFO: Spider closed (finished)
2025-06-22 11:00:34 [scrapy.utils.log] INFO: Scrapy 2.13.2 started (bot: house_spider)
2025-06-22 11:00:34 [scrapy.utils.log] INFO: Versions:
{'lxml': '5.4.0',
 'libxml2': '2.11.9',
 'cssselect': '1.3.0',
 'parsel': '1.10.0',
 'w3lib': '2.3.1',
 'Twisted': '25.5.0',
 'Python': '3.11.3 (tags/v3.11.3:f3909b8, Apr  4 2023, 23:49:59) [MSC v.1934 '
           '64 bit (AMD64)]',
 'pyOpenSSL': '25.1.0 (OpenSSL 3.5.0 8 Apr 2025)',
 'cryptography': '45.0.4',
 'Platform': 'Windows-10-10.0.22631-SP0'}
2025-06-22 11:00:34 [lianjia] INFO: 初始化爬虫，爬取页数: 1, 批次ID: 408f7973-8278-4b4e-99ab-957cad619e44
2025-06-22 11:00:34 [scrapy.addons] INFO: Enabled addons:
[]
2025-06-22 11:00:34 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.throttle.AutoThrottle']
2025-06-22 11:00:34 [scrapy.crawler] INFO: Overridden settings:
{'AUTOTHROTTLE_DEBUG': 'True',
 'AUTOTHROTTLE_ENABLED': 'True',
 'AUTOTHROTTLE_MAX_DELAY': '20',
 'AUTOTHROTTLE_START_DELAY': '8',
 'AUTOTHROTTLE_TARGET_CONCURRENCY': '0.3',
 'BOT_NAME': 'house_spider',
 'CONCURRENT_REQUESTS': '1',
 'CONCURRENT_REQUESTS_PER_DOMAIN': 2,
 'DOWNLOAD_DELAY': '8',
 'DOWNLOAD_TIMEOUT': '30',
 'LOG_FILE': 'scrapy_spider.log',
 'LOG_LEVEL': 'INFO',
 'MEMUSAGE_LIMIT_MB': 2048,
 'MEMUSAGE_WARNING_MB': 1024,
 'NEWSPIDER_MODULE': 'house_spider.spiders',
 'RANDOMIZE_DOWNLOAD_DELAY': 'True',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 408, 429],
 'RETRY_TIMES': '2',
 'SPIDER_MODULES': ['house_spider.spiders'],
 'TELNETCONSOLE_ENABLED': False}
2025-06-22 11:00:34 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'house_spider.middlewares.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'house_spider.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'house_spider.middlewares.RequestLoggingMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'house_spider.middlewares.AntiSpiderMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-06-22 11:00:34 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.start.StartSpiderMiddleware',
 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-06-22 11:00:34 [twisted] CRITICAL: Unhandled error in Deferred:
2025-06-22 11:00:34 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "E:\python\python3.11.3\Lib\site-packages\twisted\internet\defer.py", line 1857, in _inlineCallbacks
    result = context.run(gen.send, result)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "E:\python\python3.11.3\Lib\site-packages\scrapy\crawler.py", line 156, in crawl
    self.engine = self._create_engine()
                  ^^^^^^^^^^^^^^^^^^^^^
  File "E:\python\python3.11.3\Lib\site-packages\scrapy\crawler.py", line 169, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "E:\python\python3.11.3\Lib\site-packages\scrapy\core\engine.py", line 111, in __init__
    self.scraper: Scraper = Scraper(crawler)
                            ^^^^^^^^^^^^^^^^
  File "E:\python\python3.11.3\Lib\site-packages\scrapy\core\scraper.py", line 107, in __init__
    self.itemproc: ItemPipelineManager = itemproc_cls.from_crawler(crawler)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "E:\python\python3.11.3\Lib\site-packages\scrapy\middleware.py", line 77, in from_crawler
    return cls._from_settings(crawler.settings, crawler)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "E:\python\python3.11.3\Lib\site-packages\scrapy\middleware.py", line 86, in _from_settings
    mwcls = load_object(clspath)
            ^^^^^^^^^^^^^^^^^^^^
  File "E:\python\python3.11.3\Lib\site-packages\scrapy\utils\misc.py", line 71, in load_object
    mod = import_module(module)
          ^^^^^^^^^^^^^^^^^^^^^
  File "E:\python\python3.11.3\Lib\importlib\__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1206, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1178, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1128, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "<frozen importlib._bootstrap>", line 1206, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1178, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1128, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "<frozen importlib._bootstrap>", line 1206, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1178, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1142, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'mongodb_integration'
