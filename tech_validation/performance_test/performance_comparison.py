#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
æ€§èƒ½å¯¹æ¯”æµ‹è¯•è„šæœ¬
å¯¹æ¯”ç°æœ‰çˆ¬è™« vs Scrapyçˆ¬è™«çš„æ€§èƒ½å·®å¼‚
"""

import time
import psutil
import subprocess
import sys
import os
import json
from datetime import datetime
from memory_profiler import profile
import requests
from lxml import etree
import pymysql


class PerformanceTest:
    def __init__(self):
        self.results = {
            'test_time': datetime.now().isoformat(),
            'original_crawler': {},
            'scrapy_crawler': {},
            'comparison': {}
        }
        
    def test_original_crawler(self, pages=3):
        """æµ‹è¯•åŸå§‹çˆ¬è™«æ€§èƒ½"""
        print("ğŸ” æµ‹è¯•åŸå§‹çˆ¬è™«æ€§èƒ½...")
        
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # æ¨¡æ‹ŸåŸå§‹çˆ¬è™«é€»è¾‘
        headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.64 Safari/537.36'}
        
        total_items = 0
        errors = 0
        
        for page in range(1, pages + 1):
            try:
                url = f'https://gz.lianjia.com/zufang/pg{page}/#contentList'
                res = requests.get(url, headers=headers)
                html = etree.HTML(res.text)
                
                # æå–æ•°æ®
                names = html.xpath('//*[@id="content"]/div[1]/div[1]/div/div/p[1]/a/text()')
                prices = html.xpath('//*[@id="content"]/div[1]/div[1]/div/div/span/em/text()')
                
                total_items += len(names)
                time.sleep(2)  # æ¨¡æ‹Ÿå»¶è¿Ÿ
                
            except Exception as e:
                errors += 1
                print(f"é¡µé¢ {page} å‡ºé”™: {e}")
        
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        self.results['original_crawler'] = {
            'total_time': end_time - start_time,
            'total_items': total_items,
            'items_per_second': total_items / (end_time - start_time) if end_time > start_time else 0,
            'memory_usage': end_memory - start_memory,
            'errors': errors,
            'pages_tested': pages
        }
        
        print(f"âœ… åŸå§‹çˆ¬è™«æµ‹è¯•å®Œæˆ: {total_items}æ¡æ•°æ®, {end_time - start_time:.2f}ç§’")
        
    def test_scrapy_crawler(self):
        """æµ‹è¯•Scrapyçˆ¬è™«æ€§èƒ½"""
        print("ğŸ” æµ‹è¯•Scrapyçˆ¬è™«æ€§èƒ½...")
        
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # åˆ‡æ¢åˆ°scrapyé¡¹ç›®ç›®å½•
        scrapy_dir = os.path.join(os.path.dirname(__file__), '..', 'scrapy_test', 'house_spider')
        
        try:
            # è¿è¡ŒScrapyçˆ¬è™«
            result = subprocess.run([
                sys.executable, '-m', 'scrapy', 'crawl', 'lianjia', 
                '-s', 'CLOSESPIDER_PAGECOUNT=3',  # é™åˆ¶é¡µé¢æ•°
                '-o', 'test_output.json'
            ], 
            cwd=scrapy_dir, 
            capture_output=True, 
            text=True, 
            timeout=300
            )
            
            end_time = time.time()
            end_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
            
            # åˆ†æè¾“å‡º
            total_items = 0
            errors = 0
            
            # å°è¯•è¯»å–è¾“å‡ºæ–‡ä»¶
            output_file = os.path.join(scrapy_dir, 'test_output.json')
            if os.path.exists(output_file):
                try:
                    with open(output_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        total_items = len(data) if isinstance(data, list) else 1
                except:
                    pass
            
            # ä»æ—¥å¿—ä¸­æå–ç»Ÿè®¡ä¿¡æ¯
            if result.stderr:
                log_lines = result.stderr.split('\n')
                for line in log_lines:
                    if 'item_scraped_count' in line:
                        try:
                            total_items = int(line.split('item_scraped_count')[1].split(',')[0].strip().split(':')[1])
                        except:
                            pass
            
            self.results['scrapy_crawler'] = {
                'total_time': end_time - start_time,
                'total_items': total_items,
                'items_per_second': total_items / (end_time - start_time) if end_time > start_time else 0,
                'memory_usage': end_memory - start_memory,
                'errors': errors,
                'return_code': result.returncode,
                'stdout': result.stdout[:500] if result.stdout else '',
                'stderr': result.stderr[:500] if result.stderr else ''
            }
            
            print(f"âœ… Scrapyçˆ¬è™«æµ‹è¯•å®Œæˆ: {total_items}æ¡æ•°æ®, {end_time - start_time:.2f}ç§’")
            
        except subprocess.TimeoutExpired:
            print("âŒ Scrapyæµ‹è¯•è¶…æ—¶")
            self.results['scrapy_crawler'] = {
                'error': 'timeout',
                'total_time': 300,
                'total_items': 0
            }
        except Exception as e:
            print(f"âŒ Scrapyæµ‹è¯•å‡ºé”™: {e}")
            self.results['scrapy_crawler'] = {
                'error': str(e),
                'total_time': 0,
                'total_items': 0
            }
    
    def compare_results(self):
        """å¯¹æ¯”æµ‹è¯•ç»“æœ"""
        print("ğŸ“Š ç”Ÿæˆæ€§èƒ½å¯¹æ¯”æŠ¥å‘Š...")
        
        original = self.results['original_crawler']
        scrapy = self.results['scrapy_crawler']
        
        if 'error' not in scrapy and 'error' not in original:
            self.results['comparison'] = {
                'speed_improvement': (scrapy['items_per_second'] / original['items_per_second'] - 1) * 100 if original['items_per_second'] > 0 else 0,
                'time_difference': scrapy['total_time'] - original['total_time'],
                'memory_difference': scrapy['memory_usage'] - original['memory_usage'],
                'reliability_improvement': (original['errors'] - scrapy['errors']) / max(original['errors'], 1) * 100
            }
        
    def generate_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        report_file = os.path.join(os.path.dirname(__file__), '..', 'reports', 'performance_comparison.json')
        os.makedirs(os.path.dirname(report_file), exist_ok=True)
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        # ç”Ÿæˆå¯è¯»æŠ¥å‘Š
        readable_report = self.generate_readable_report()
        report_md_file = os.path.join(os.path.dirname(report_file), 'performance_comparison.md')
        
        with open(report_md_file, 'w', encoding='utf-8') as f:
            f.write(readable_report)
        
        print(f"ğŸ“„ æŠ¥å‘Šå·²ç”Ÿæˆ: {report_md_file}")
        
    def generate_readable_report(self):
        """ç”Ÿæˆå¯è¯»çš„MarkdownæŠ¥å‘Š"""
        original = self.results.get('original_crawler', {})
        scrapy = self.results.get('scrapy_crawler', {})
        comparison = self.results.get('comparison', {})
        
        report = f"""# çˆ¬è™«æ€§èƒ½å¯¹æ¯”æµ‹è¯•æŠ¥å‘Š

## æµ‹è¯•æ—¶é—´
{self.results['test_time']}

## æµ‹è¯•ç»“æœ

### åŸå§‹çˆ¬è™« (requests + lxml)
- **æ€»è€—æ—¶**: {original.get('total_time', 0):.2f} ç§’
- **æŠ“å–æ•°æ®**: {original.get('total_items', 0)} æ¡
- **æŠ“å–é€Ÿåº¦**: {original.get('items_per_second', 0):.2f} æ¡/ç§’
- **å†…å­˜ä½¿ç”¨**: {original.get('memory_usage', 0):.2f} MB
- **é”™è¯¯æ•°é‡**: {original.get('errors', 0)} ä¸ª

### Scrapyçˆ¬è™«
- **æ€»è€—æ—¶**: {scrapy.get('total_time', 0):.2f} ç§’
- **æŠ“å–æ•°æ®**: {scrapy.get('total_items', 0)} æ¡
- **æŠ“å–é€Ÿåº¦**: {scrapy.get('items_per_second', 0):.2f} æ¡/ç§’
- **å†…å­˜ä½¿ç”¨**: {scrapy.get('memory_usage', 0):.2f} MB
- **é”™è¯¯æ•°é‡**: {scrapy.get('errors', 0)} ä¸ª

## æ€§èƒ½å¯¹æ¯”

"""
        
        if comparison:
            report += f"""- **é€Ÿåº¦æå‡**: {comparison.get('speed_improvement', 0):.1f}%
- **æ—¶é—´å·®å¼‚**: {comparison.get('time_difference', 0):.2f} ç§’
- **å†…å­˜å·®å¼‚**: {comparison.get('memory_difference', 0):.2f} MB
- **å¯é æ€§æå‡**: {comparison.get('reliability_improvement', 0):.1f}%
"""
        
        report += f"""
## ç»“è®º

{'Scrapyåœ¨æ€§èƒ½å’Œå¯é æ€§æ–¹é¢è¡¨ç°æ›´ä¼˜' if comparison.get('speed_improvement', 0) > 0 else 'éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–Scrapyé…ç½®'}

## å»ºè®®

1. å¦‚æœScrapyæ€§èƒ½æ›´ä¼˜ï¼Œå»ºè®®è¿›è¡Œæ¶æ„å‡çº§
2. å¦‚æœæ€§èƒ½ç›¸è¿‘ï¼Œè€ƒè™‘Scrapyçš„å…¶ä»–ä¼˜åŠ¿ï¼ˆå»é‡ã€é”™è¯¯å¤„ç†ã€æ‰©å±•æ€§ï¼‰
3. å»ºè®®è¿›è¡Œæ›´å¤§è§„æ¨¡çš„æµ‹è¯•éªŒè¯
"""
        
        return report
    
    def run_full_test(self):
        """è¿è¡Œå®Œæ•´æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹æ€§èƒ½å¯¹æ¯”æµ‹è¯•...")
        
        # æµ‹è¯•åŸå§‹çˆ¬è™«
        self.test_original_crawler(pages=3)
        
        # æµ‹è¯•Scrapyçˆ¬è™«
        self.test_scrapy_crawler()
        
        # å¯¹æ¯”ç»“æœ
        self.compare_results()
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_report()
        
        print("âœ… æ€§èƒ½æµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    test = PerformanceTest()
    test.run_full_test()
